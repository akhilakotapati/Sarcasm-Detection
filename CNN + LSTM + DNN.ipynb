{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jai Ganesh Deva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network Followed by Gru and DNN\n",
    "\n",
    "Replacing LSTM with a Bidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vamsi\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\vamsi\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\vamsi\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\vamsi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize  import word_tokenize\n",
    "import nltk\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import pipeline,metrics, grid_search\n",
    "from gensim import models, corpora\n",
    "from gensim.models import ldamodel\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import collections\n",
    "import io\n",
    "import keras.backend as K\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dill as pickle\n",
    "import gensim\n",
    "from keras.optimizers import Adam\n",
    "from gensim.models import word2vec,doc2vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_union\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import  Dense, Input,Conv1D,Convolution1D,GRU,Convolution2D ,LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, Masking,MaxPooling1D,GlobalMaxPool1D,Bidirectional,Merge,Flatten,SpatialDropout1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"E:\\\\Datasets\\\\Final Project\\\\Politics Final\\\\test_balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"train_ndf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,     0,   285,   669],\n",
       "       [    0,     0,     0, ...,     2,     4,  5379],\n",
       "       [    0,     0,     0, ...,    18,    84,   748],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,    43,     3, 15446],\n",
       "       [    0,     0,     0, ...,     7,   639,  4854],\n",
       "       [    0,     0,     0, ...,     0,     0,  2520]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"test_ndf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(['Unnamed: 0', 'Unnamed: 0.1'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"text_tokens_stop\"] = X_train[\"text_tokens_stop\"].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"lower_wp\"] = X_test[\"lower_wp\"].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = np.array(X_train[\"sarcastic or not\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Y = np.array(X_test[\"sarcastic or not\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = X_train[\"lower_wp\"]\n",
    "list_sentences_test = X_test[\"lower_wp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "   \n",
    "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "   \n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features,char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(list(X_train[\"lower_wp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_sentences_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_sentences_test, maxlen=maxlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.add(Embedding(len(tokenizer.word_index)+1,embed_size,input_length=maxlen))\n",
    "model.add(Conv1D(filters = 64,kernel_size=3,padding = \"same\",activation = \"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Bidirectional(GRU(45, return_sequences=True,name='lstm_layer',dropout=0.4,recurrent_dropout=0.4)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1,activation = \"sigmoid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizers.RMSprop(),\n",
    "                 metrics=['accuracy',precision,recall, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, 200, 200)          2640000   \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 200, 64)           38464     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 100, 90)           29700     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_22 (Glo (None, 90)                0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 50)                4550      \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 2,712,765\n",
      "Trainable params: 2,712,765\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12301 samples, validate on 1367 samples\n",
      "Epoch 1/4\n",
      "12301/12301 [==============================] - 63s 5ms/step - loss: 0.6680 - acc: 0.5832 - precision: 0.5939 - recall: 0.6088 - f1: 0.5853 - val_loss: 0.6307 - val_acc: 0.6533 - val_precision: 0.6688 - val_recall: 0.6063 - val_f1: 0.6337\n",
      "Epoch 2/4\n",
      "12301/12301 [==============================] - 40s 3ms/step - loss: 0.5811 - acc: 0.7051 - precision: 0.7235 - recall: 0.6749 - f1: 0.6932 - val_loss: 0.6135 - val_acc: 0.6635 - val_precision: 0.6974 - val_recall: 0.5760 - val_f1: 0.6279\n",
      "Epoch 3/4\n",
      "12301/12301 [==============================] - 38s 3ms/step - loss: 0.4983 - acc: 0.7655 - precision: 0.7689 - recall: 0.7660 - f1: 0.7627 - val_loss: 0.6286 - val_acc: 0.6620 - val_precision: 0.6476 - val_recall: 0.7123 - val_f1: 0.6773\n",
      "Epoch 00003: early stopping\n"
     ]
    }
   ],
   "source": [
    "hist5 = model.fit(X_t,train_Y, batch_size=64, epochs=epochs,validation_split=0.1,callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_classes(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6746478873239437\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.62      0.65      1420\n",
      "          1       0.66      0.73      0.69      1420\n",
      "\n",
      "avg / total       0.68      0.67      0.67      2840\n",
      "\n",
      "[[ 876  544]\n",
      " [ 380 1040]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "\n",
    "acc = accuracy_score(test_Y,pred)\n",
    "print(acc)\n",
    "print(classification_report(test_Y,pred))\n",
    "print(confusion_matrix(y_true = test_Y,y_pred= pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(\"cnn+lstm+basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=1, \\\n",
    "                          verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17080 samples, validate on 1898 samples\n",
      "Epoch 1/4\n",
      "17080/17080 [==============================] - 67s 4ms/step - loss: 0.4433 - acc: 0.8006 - precision: 0.8137 - recall: 0.8339 - f1: 0.8186 - val_loss: 0.8945 - val_acc: 0.5337 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1: nan\n",
      "Epoch 2/4\n",
      "17080/17080 [==============================] - 65s 4ms/step - loss: 0.3110 - acc: 0.8715 - precision: 0.8772 - recall: 0.8973 - f1: 0.8836 - val_loss: 0.9386 - val_acc: 0.5590 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1: nan\n",
      "Epoch 3/4\n",
      "17080/17080 [==============================] - 65s 4ms/step - loss: 0.1931 - acc: 0.9248 - precision: 0.9287 - recall: 0.9366 - f1: 0.9304 - val_loss: 0.9630 - val_acc: 0.6523 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1: nan\n",
      "Epoch 4/4\n",
      "17080/17080 [==============================] - 66s 4ms/step - loss: 0.1249 - acc: 0.9529 - precision: 0.9545 - recall: 0.9607 - f1: 0.9564 - val_loss: 1.0926 - val_acc: 0.6581 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1: nan\n"
     ]
    }
   ],
   "source": [
    "hist4 = model.fit(X_t,train_Y, batch_size=batch_size, epochs=epochs,validation_split=0.1,callbacks=[earlystop])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
