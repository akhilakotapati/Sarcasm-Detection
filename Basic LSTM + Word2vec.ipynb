{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jai Ganesh Deva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize  import word_tokenize\n",
    "import nltk\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import pipeline,metrics, grid_search\n",
    "from gensim import models, corpora\n",
    "from gensim.models import ldamodel\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import collections\n",
    "import io\n",
    "import keras.backend as K\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dill as pickle\n",
    "import gensim\n",
    "from keras.optimizers import Adam\n",
    "from gensim.models import word2vec,doc2vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_union\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import  Dense, Input,Conv1D,Convolution1D,Convolution2D ,LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, Masking,MaxPooling1D,GlobalMaxPool1D,Bidirectional,Merge,Flatten,Reshape\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"E:\\Datasets\\Final Project\\Politics Final\\Big\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"train_Bigdf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"test_Bigdf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4674"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = np.array(X_train[\"sarcastic or not\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Y = np.array(X_test[\"sarcastic or not\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "   \n",
    "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "   \n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"text_tokens_stop\"] = X_train[\"text_tokens_stop\"].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"text_tokens_stop\"] = X_test[\"text_tokens_stop\"].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in X_train[\"text_tokens_stop\"]:\n",
    "    sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in X_test[\"text_tokens_stop\"]:\n",
    "    sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_small = word2vec.Word2Vec(sentences,sg=1,window=4,size=100,\n",
    "                 min_count=2,workers=5,iter=100,sample=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec_small.wv.save_word2vec_format(\"word2vec_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16757 unique tokens.\n",
      "(16758, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vamsi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\vamsi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=30000, lower=False, char_level=False)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "training_sequences = tokenizer.texts_to_sequences(X_train[\"lower_wp\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=200)\n",
    "\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, 100))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec_small[word] if word in word2vec_small else np.random.rand(100)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(X_test[\"lower_wp\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ..., 10656, 13095, 10656],\n",
       "       [    0,     0,     0, ...,     0,     0,     0],\n",
       "       [    0,     0,     0, ...,     0,     0, 12974],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,     0, 13095, 10656],\n",
       "       [    0,     0,     0, ...,     0,     0,     0],\n",
       "       [    0,     0,     0, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cnn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,     0,     0,     0],\n",
       "       [    0,     0,     0, ...,     0,     0,     0],\n",
       "       [    0,     0,     0, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,     0,     0,     0],\n",
       "       [    0,     0,     0, ...,     0,     0, 10656],\n",
       "       [    0,     0,     0, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cnn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.add(Embedding(len(train_word_index)+1,100,input_length=200,weights = [train_embedding_weights]))\n",
    "model.add(LSTM(128,recurrent_dropout=0.4))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(30,name='FC1',kernel_initializer='normal',kernel_regularizer='l2'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_acc', patience=10,mode='max',verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 200, 100)          1675800   \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 30)                3870      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 31        \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,797,461\n",
      "Trainable params: 1,797,205\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist1 = model.fit(train_cnn_data,train_Y,epochs = 10, batch_size=32,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_classes(test_cnn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "\n",
    "acc = accuracy_score(test_Y,pred)\n",
    "print(acc)\n",
    "print(classification_report(test_Y,pred))\n",
    "print(confusion_matrix(y_true = test_Y,y_pred= pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
